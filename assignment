#######################################
Deadline: Montag, 15. April 2013, 23:55
#######################################


General Information

Note: submission of this assignment counts as valid registration to the course (i.e., you will receive an overall grade in the end then).
Implement your solution with plain Java (common libs for I/O, CLIs, collections & data structures and the likes can be used).
Build system/tools etc. are free choice; You have to use a publicly accessible version control system (SVN, GIT, ....), and add Mihai Lupup (lupu@ifs.tuwien.ac.at, mihailupu@gmail.com) and Rudolf Mayer (mayer@ifs.tuwien.ac.at, rudolf.mayer@gmail.com) to your project.
Submit your source code plus a corresponding executable file (JAR, bin, ...) incl. all dependencies. In other words, your submission has to be self-contained!
Also include howto run instructions and short documentation as an additional plaintext README.
Deliverable is a Zip archive to be uploaded via TUWEL, containing all relevant project files, as well as the output files!
CLI-based (i.e., shell) apps are sufficient (no GUI, Web interface necessary).
Output shall be basic ARFF (Attribute-Relation File Format) files; besides the features, you shall include one field with the name of the document, and one field with the class of the document (see below). To ensure compability, please install http://www.cs.waikato.ac.nz/ml/weka/ and check that your generated ARFF file opens w/o problems in the WEKA Explorer
You are free to reuse existing libraries for tasks such as stemming or writing ARFF files, or for efficient data structures (sparse matrices, etc.). The basic components of the indexer have to be developed by yourself.

BTW, keep in mind that you will continue working with your solution for two more exercises.


You need to sign up for the group at https://tuwel.tuwien.ac.at/mod/grouptool/view.php?id=140542
You may, e.g., use the TUWEL forum of the course for finding group members; please, keep in mind that workload/grading doesn't take into account if 1 or 2 persons worked on the assignment, though. You can keep the same groups for the rest of the exercises.


FYI: each of the mentioned available techniques are presented during the lectures as well as extensive information can be found on the Web (hint: if applicable do some scientific literature research). Consequently, concrete details connected to ways of implementation are deliberately left open. However, any questions and/or (constructive) discussions are very welcome in the TUWEL forum. 



Task 1.1

Develop a simple text indexer and feature extractor, based on the information from the lectures, using the Bag-of-Words approach with TF*IDF (Inverse Document Frequency) weighting

As text document corpus to feed your indexer you will be provided with a subset of the 20-newsgroups data set, and available at https://tuwel.tuwien.ac.at/mod/resource/view.php?id=140543

This corpus contains a set of classes, and each document is assigned to exactly one class. The assignment in this case is implicitly with the folder structure. Your WEKA output file should contain as one attribute this class assignment.

Your indexer should also allow optional stemming of the documents, and allow frequency thresholding for lower and upper bounds. For your submission, you shall provide three different files with a different numbers of terms, using different frequency thresholds. You shall provide one file with few, one with medium and one with as-many-as-possible terms.

Make use of the feature to gzip compress the ARFF files, to reduce the overall file size. The thresholds should be user-controlled parameters to your program. 



Task 1.2


In the second part, you shall perform similarity retrieval on the two document collections used before.

Input: 
* your indexer created in Task 1.1
* a set of 20 topics for the postings (20 newsgroups) text collection

Each topic is a document filename (without extension) meaning that you are required to find the set of documents most similar to the given one. You are free to use the full text as a query, or to create your own set of queries, as you see fit. 

Output: for each topic, the top 10 most similar documents of the corresponding collection.

Expected work (for each of the three postings lists (small, medium, large, see above))

1. index the text collection 
2. implement a similarity measure (see chapters 6, 7) based on your indexer
3. return the top 10 most similar documents to each topic using your own similarity measure

Output format:
We expect, from each group, a set of 60 plain-text files, with a particular filename and content. 

Filename: <postingListSize>_topic<XX>_group<YY>.txt
where 
- postingListSize can be small | medium | large
- XX is a number identifying the topic, between 1 and 20 (do not use prefix with 0s)
- YY is a number identifying your working group
Example: medium_topic1_group10.txt

Content: each file contains exactly 10 lines. Each line has the following syntax:
topic<XX> Q0 <doc_id> <rank> <similarity> group<YY>_<postingListSize>
The 10 lines in the file are ranked in decreasing order on similarity (i.e. the first line contains the doc_id of the document most similar to the topic, the second line - the second most similar and so forth). This should mean a monotonically increasing ordering on the value. 

The doc_id is simply the filename (without extension). 

Example of the content of a file:
topic1 Q0 misc.forsale/74721 1 34.32 group10_medium
topic1 Q0 misc.forsale/74730 2 30.12 group10_medium
topic1 Q0 misc.forsale/74741 3 28.99 group10_medium
...
topic1 Q0 misc.forsale/747 10 0.99 group10_medium

The topics are :

misc.forsale/76057
talk.religion.misc/83561
talk.politics.mideast/75422
sci.electronics/53720
sci.crypt/15725
misc.forsale/76165
talk.politics.mideast/76261
alt.atheism/53358
sci.electronics/54340
rec.motorcycles/104389
talk.politics.guns/54328
misc.forsale/76468
sci.crypt/15469
rec.sport.hockey/54171
talk.religion.misc/84177
rec.motorcycles/104727
comp.sys.mac.hardware/52165
sci.crypt/15379
sci.space/60779
sci.med/59456
